{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 DATA ACQUISITION AND UDERSTANDING\n",
    "\n",
    "## Goals\n",
    "\n",
    "Produce a clean, high-quality data set whose relationship to the target variables is understood. Locate the data set in the appropriate analytics environment so you are ready to model.\n",
    "Develop a solution architecture of the data pipeline that refreshes and scores the data regularly.\n",
    "\n",
    "## How to do it\n",
    "\n",
    "There are three main tasks addressed in this stage:\n",
    "\n",
    "0. Ingest the data into the target analytic environment.\n",
    "0. Explore the data to determine if the data quality is adequate to answer the question.\n",
    "0. Set up a data pipeline to score new or regularly refreshed data.\n",
    "\n",
    "Our source: https://ourworldindata.org/covid-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from darkstyle import dark_style as dks\n",
    "\n",
    "# statistic\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# scalers\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# feature selection\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# model's cross-validation\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict, HalvingGridSearchCV\n",
    "\n",
    "# model's metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# save files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some default figure paramenters and style\n",
    "def settings():\n",
    "    dks.dark_style() # module for matplot darkstyle\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "settings()\n",
    "\n",
    "# cramers v statistic function for categorical values\n",
    "def cramers_corrected_stat(x, y):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    cm = pd.crosstab(x, y).values\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "\n",
    "    # Calculate chi2\n",
    "    chi2 = stats.chi2_contingency(cm)[0]\n",
    "    # Calculate chi2 correction\n",
    "    chi2corr = max(0, chi2 - (k-1)*(r-1)/(n-1))\n",
    "    # K correction\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    # R correction\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    return np.sqrt((chi2corr/n) / (min(kcorr-1, rcorr-1)))\n",
    "\n",
    "# model's performance function\n",
    "def error(model, y_test, y_pred):\n",
    "    root_mse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rs = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return pd.DataFrame( {\n",
    "        'Model' : model,\n",
    "        'MAE' : mae,\n",
    "        'R2' : rs,\n",
    "        'RMSE' : root_mse\n",
    "           }, index=[0])\n",
    "\n",
    "# cross Validation Function\n",
    "def cross_validation(model_name, model, x, y):\n",
    "\n",
    "    # Error lists to concatenate the values\n",
    "    rmse_list = cross_val_score(model, x, y, scoring='neg_root_mean_squared_error', cv=5)\n",
    "    mae_list = cross_val_score(model, x, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "    r2_list = cross_val_score(model, x, y, scoring='r2', cv=5)\n",
    "\n",
    "    return pd.DataFrame( {\n",
    "        'Model Name' : model_name,\n",
    "        'MAE' : np.round(np.mean(-mae_list), 4),\n",
    "        'MAE STD' : np.round(np.std(mae_list), 4),\n",
    "        'R2' : np.mean(np.mean(r2_list)),\n",
    "        'RSME' : np.round(np.mean(-rmse_list), 4),\n",
    "        'RSME STD' : np.round(np.std(rmse_list), 4)\n",
    "    }, index=[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading main file\n",
    "url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# load describe columns file\n",
    "cols_describe = pd.read_csv('dataset/describe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Knowing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The dataset shape is: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Type and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some features has Null values;\n",
    "* dtypes: float64(54), object(5);\n",
    "* Total entries: 75558;\n",
    "* Total of 59 columns;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features has many null values, the drop technique was not applied because the missing values are due to events that have not occurred yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking miss values rate\n",
    "missing_values = []\n",
    "for col in df.columns:\n",
    "    total = len(df[col])\n",
    "    total_missing = df[col].isna().sum()\n",
    "    missing_rate = total_missing/total\n",
    "    # append to list\n",
    "    if missing_rate > 0.6:\n",
    "        missing_values.append(col)\n",
    "\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with too much missing values\n",
    "#df2 = df.drop(columns = columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1. Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting numerical attributes\n",
    "num_df = df.select_dtypes(include=['float64'])\n",
    "\n",
    "# describe\n",
    "describe = num_df.describe().T\n",
    "\n",
    "# adding other metrics to knowing data\n",
    "describe['range'] = (num_df.max() - num_df.min()).tolist()\n",
    "describe['unique val.'] = num_df.nunique()\n",
    "describe['variation coefficient'] = np.round((num_df.std() / num_df.mean()), 4).tolist()\n",
    "describe['skew'] = np.round(num_df.skew(), 4).tolist()\n",
    "describe['kurtosis'] = np.round(num_df.kurtosis(), 4).tolist()\n",
    "\n",
    "describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about skewness\n",
    "high_skewness = []\n",
    "for feat in range(len(describe.index)):\n",
    "    if abs(describe['skew'].iloc[feat]) > 2:\n",
    "        high_skewness.append(describe.index[feat])\n",
    "print(f'There\\'s {len(high_skewness)} features with high skew:')\n",
    "print(high_skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about kurstosis\n",
    "high_kurstosis = []\n",
    "for feat in range(len(describe.index)):\n",
    "    if abs(describe['kurtosis'].iloc[feat]) > 3:\n",
    "        high_kurstosis.append(describe.index[feat])\n",
    "print(f'There\\'s {len(high_kurstosis)} features with high kurtosis:')\n",
    "print(high_kurstosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative values\n",
    "negative_values = []\n",
    "for feat in range(len(describe.index)):\n",
    "    if abs(describe['min'].iloc[feat] <= 0):\n",
    "        negative_values.append(describe.index[feat])\n",
    "print(f'There\\'s {len(negative_values)} features with negative values:')\n",
    "print(negative_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative values have no meaning to events. It was, probably, a typo. \n",
    "Negative values often seem very distant from behavior, let's replace the values with 0.\n",
    "The skewness and high kurtosis values, we will dealing with those features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace negative values with mean\n",
    "for i in negative_values:\n",
    "    df[i] = df[i].apply(lambda x : x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2. Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df.select_dtypes(exclude='float64')\n",
    "\n",
    "cat_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is 215 countries in the present dataset;\n",
    "* 6 continents;\n",
    "* continents have null values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0. Feature Engineering and Hypothesis Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform object in datetime format\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "# creating year column\n",
    "df['year'] = df.date.dt.year\n",
    "\n",
    "# creating intervals\n",
    "df['year_month'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting by year and month\n",
    "df = df.sort_values('year_month').reset_index(drop=True)\n",
    "\n",
    "# removing wordwild entries\n",
    "df = df[~df['iso_code'].str.contains('OWID')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis summary\n",
    "\n",
    "| INDEX | HYPOTHESIS                                                   |\n",
    "| ----- | ------------------------------------------------------------ |\n",
    "| H1    | Brazil more likely to Covid19 than USA in total cases.       |\n",
    "| H2    | Continent's with high vaccination rate are more effective on Covid19 control. |\n",
    "| H3    | Countries with high populatation density are more likely to Covid19. |\n",
    "| H4    | Countries with high elderly are most affected by Covid19.    |\n",
    "| H5    | Countries with high GDP are less likely to covid19.          |\n",
    "| H6    | USA has more Covid19 death.                                  |\n",
    "| H7    | European continent conducted the highest number of tests.    |\n",
    "| H8    | Brazil is more likely to new cases of Covid19.               |\n",
    "| H9    | USA is more likely to vaccination.                           |\n",
    "| H10   | Brazil is more likely Covid19 in South America.              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Target Variable\n",
    "\n",
    "Our target variable is total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# plot the density   \n",
    "sns.histplot(df.total_cases, kde=True, ax=ax[0])\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot the boxplot   \n",
    "sns.boxplot(df.total_cases, ax=ax[1])\n",
    "ax[1].set_xlabel('Value')\n",
    "\n",
    "# plot the lineplot\n",
    "sns.lineplot(data=df, x='year_month', y='total_cases', ax=ax[2])\n",
    "ax[2].set_title('Trend')\n",
    "\n",
    "# Add a title to the Figure\n",
    "fig.suptitle('Data Distribution')\n",
    "\n",
    "# savefig\n",
    "#plt.savefig('img/fig00a')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plot the density   \n",
    "sns.histplot(np.log(df.total_cases), kde=True)\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "# savefig\n",
    "#plt.savefig('img/fig00b')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting numerical with filtering values\n",
    "num_df2 = df.select_dtypes(include='float64')\n",
    "\n",
    "# removing total_cases column\n",
    "numerical = num_df2.drop('total_cases', axis=1)\n",
    "\n",
    "# plot \n",
    "numerical.hist(bins=25)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('img/fig01')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing total_cases \n",
    "high_skewness.remove('total_cases')\n",
    "\n",
    "# selecting columns with high skew\n",
    "numerical_selected = numerical[high_skewness]\n",
    "\n",
    "# removing negative values\n",
    "no_neg = numerical_selected[numerical_selected >= 0]\n",
    "\n",
    "# plot\n",
    "np.log1p(numerical_selected).hist(bins=25)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('img/fig02')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the initial plot position\n",
    "n = 1\n",
    "\n",
    "# iterate over the columns to plot\n",
    "for column in numerical.columns:\n",
    "    plt.subplot(8, 7, n)\n",
    "    _ = sns.boxplot(numerical[column])\n",
    "    n += 1\n",
    "    \n",
    "# adjusts vertical space between plots\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "#plt.savefig('img/fig03')\n",
    "\n",
    "# displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping features with high kurtosis\n",
    "high_outliers = list(set(high_kurstosis)| set(high_skewness))\n",
    "print('Columns with high outliers influence:')\n",
    "print(high_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplot 1\n",
    "plt.subplot(2,2,1)\n",
    "sns.countplot(cat_df.iso_code)\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(2,2,2)\n",
    "sns.countplot(cat_df.continent)\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(2,2,3)\n",
    "sns.countplot(cat_df.location)\n",
    "\n",
    "# subplot 4\n",
    "plt.subplot(2,2,4)\n",
    "sns.countplot(cat_df.tests_units)\n",
    "\n",
    "# save plot\n",
    "#plt.savefig('img/fig04')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving optimize dataset\n",
    "df.to_csv('dataset/df1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint 1\n",
    "df2 = pd.read_csv('dataset/df1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1. Brazil more likely to Covid19 than USA in total cases. (TRUE)\n",
    "Brazil has a smaller population and has more cases than USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features\n",
    "auxh1 = df2[['iso_code', 'population', 'year_month', 'total_cases', 'total_cases_per_million']]\n",
    "\n",
    "# selecting countries\n",
    "bra_usa =  auxh1.query('(iso_code == \"BRA\") | (iso_code == \"USA\")').sort_values('year_month')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "# subplot 1\n",
    "sns.barplot(data=bra_usa, x='iso_code', y='population', ax= ax[0])\n",
    "ax[0].set_title('USA X BRA POPULATION')\n",
    "\n",
    "# subplot 2\n",
    "sns.lineplot(data=bra_usa, x='year_month', y='total_cases', hue='iso_code', ci=None, ax=ax[1])\n",
    "ax[1].tick_params(rotation=45)\n",
    "ax[1].set_title('TOTAL CASES')\n",
    "\n",
    "# subplot 3\n",
    "sns.lineplot(data=bra_usa, x='year_month', y='total_cases_per_million', hue='iso_code', ci=None, ax=ax[2])\n",
    "ax[2].tick_params(rotation=45)\n",
    "ax[2].set_title('TOTAL CASES PER MILLION')\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig05')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2. Continentes with high vaccination rate are more effective on Covid19 control. (PERHAPS)\n",
    "There are still not enough elements since the vaccine is a recent event, but the continents with the highest numbers of cases are vaccinating more.\n",
    "A strong correlation between total cases and total vaccine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh2 = df2[['continent', 'year_month', 'date', 'total_cases', 'total_vaccinations',\n",
    "       'total_cases_per_million', 'total_vaccinations_per_hundred']]\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(2,3,1)\n",
    "sns.barplot(data=auxh2, x='continent', y='total_cases')\n",
    "plt.title('CONTINENTS TOTAL CASE')\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(2,3,2)\n",
    "sns.lineplot(data=auxh2, hue='continent', x='year_month', y='total_vaccinations', ci=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('TOTAL VACCINATION')\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(2,3,3)\n",
    "sns.heatmap(auxh2[['total_cases', 'total_vaccinations']].corr(method='pearson'), annot=True)\n",
    "\n",
    "# subplot 4\n",
    "plt.subplot(2,3,4)\n",
    "sns.barplot(data=auxh2, x='continent', y='total_cases_per_million')\n",
    "plt.title('CONTINENTS TOTAL CASE PER MILLION')\n",
    "\n",
    "# subplot 5\n",
    "plt.subplot(2,3,5)\n",
    "sns.lineplot(data=auxh2, hue='continent', x='year_month', y='total_vaccinations_per_hundred', ci=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('TOTAL VACCINATIONS PER HUNDRED')\n",
    "\n",
    "# subplot 6\n",
    "plt.subplot(2,3,6)\n",
    "sns.heatmap(auxh2[['total_cases', 'total_vaccinations_per_hundred']].corr(method='pearson'), annot=True)\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig06')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3. Countries with high populatation density are more likely to Covid19. (FALSE)\n",
    "There is a weak correlation between the total number of cases and population density. In top 5 countries with higher cases, only Indian has high population density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting columns\n",
    "auxh3 = df2[['iso_code', 'total_cases', 'population_density']]\n",
    "\n",
    "# removing worldwide variable and groupby countries\n",
    "auxh3_gp = auxh3.groupby('iso_code').sum().reset_index()\n",
    "\n",
    "# sorting values\n",
    "auxh3_gp = auxh3_gp.sort_values('total_cases', ascending=False)\n",
    "\n",
    "# initialize figure\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# load total_cases\n",
    "sns.barplot(data=auxh3_gp.head(50), x='iso_code', y='total_cases', color='w', ax=ax[0])\n",
    "ax[0].set_title('COVID 19 HIGHEST CASE TOP 50')\n",
    "\n",
    "# plot the population density\n",
    "sns.barplot(data = auxh3_gp.head(50), x='iso_code', y='population_density', color='y', ax=ax[1])\n",
    "ax[1].invert_yaxis()\n",
    "ax[1].set_title('POPULATION DENSITY')\n",
    "\n",
    "# heatmap\n",
    "sns.heatmap(auxh3[['total_cases', 'population_density']].corr(method='pearson'), annot=True, ax=ax[2])\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig07')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4. Countries with high elderly are most affected by Covid19. (FALSE)\n",
    "Most countries with high rate of elderly people have low rate of total cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh4 = df2[['iso_code', 'total_cases', 'aged_65_older', 'aged_70_older']]\n",
    "\n",
    "auxh4['elderly'] = auxh4['aged_65_older'] + auxh4['aged_70_older']\n",
    "\n",
    "auxh4_gp = auxh4.groupby('iso_code').mean().reset_index()\n",
    "\n",
    "# sorting values\n",
    "auxh4_gp = auxh4_gp.sort_values('total_cases', ascending=False)\n",
    "\n",
    "# set fig\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1)\n",
    "\n",
    "# ax 1\n",
    "sns.barplot(data=auxh4_gp.head(50), x='iso_code', y='elderly',\\\n",
    "    color='cyan', ax=ax[0])\n",
    "ax[1].set_title('ELDERLY RATE')\n",
    "\n",
    "# ax 2\n",
    "sns.barplot(data=auxh4_gp.head(50), x='iso_code', y='total_cases',\\\n",
    "    color='white', ax=ax[1])\n",
    "ax[1].invert_yaxis()\n",
    "ax[1].set_title('TOTAL CASES')\n",
    "\n",
    "# ax 3\n",
    "sns.heatmap(auxh4[['total_cases', 'elderly']].corr(method='pearson'), annot=True, ax=ax[2])\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig08')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5. Countries with high GDP are less likely to covid19. (FALSE)\n",
    "Countries are being affected regardless of their GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh5 = df2[['iso_code', 'total_cases', 'gdp_per_capita', 'extreme_poverty']]\n",
    "\n",
    "auxh5_gp = auxh5[~auxh5.iso_code.str.contains('OWID')].groupby('iso_code').sum().reset_index()\n",
    "\n",
    "# sorting values\n",
    "auxh5_gp = auxh5_gp.sort_values('total_cases', ascending=False)\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(data=auxh5_gp.head(30), x='iso_code', y='gdp_per_capita')\n",
    "plt.title('GDP PER COUNTRY')\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(data=auxh5_gp, x='total_cases', y='extreme_poverty')\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(data=auxh5_gp.head(30), x='iso_code', y='extreme_poverty')\n",
    "plt.title('EXTREME POVERTY')\n",
    "\n",
    "# subplot 4\n",
    "plt.subplot(2,2,4)\n",
    "sns.heatmap(auxh5[['total_cases', 'gdp_per_capita', 'extreme_poverty']].corr(method='pearson'), annot=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig09')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H6. USA has more Covid19 death. (TRUE)\n",
    "USA leads total death toll per covid and has the most deaths per million in north america."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh6 = df2[['iso_code', 'continent', 'total_deaths', 'total_cases', 'total_deaths_per_million', 'year_month']]\n",
    "\n",
    "auxh6_gp = auxh6[~auxh6.iso_code.str.contains('OWID')].groupby('iso_code').sum().reset_index()\n",
    "\n",
    "auxh6_gp = auxh6_gp.sort_values('total_deaths', ascending=False)\n",
    "\n",
    "# fig ax\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 1)\n",
    "\n",
    "# plot 1\n",
    "sns.barplot(data=auxh6_gp.head(50), y='total_deaths', x='iso_code', ax=ax[0])\n",
    "\n",
    "# plot 2\n",
    "#sns.barplot(data=auxh6_gp.head(50), y='total_cases', x='iso_code', ax=ax[1])\n",
    "#ax[1].invert_yaxis()\n",
    "\n",
    "# plot 3\n",
    "north_america = df2.query('continent == \"North America\"')\n",
    "sns.lineplot(data=north_america, x='year_month', y='total_deaths_per_million',\\\n",
    "     hue='iso_code', style='iso_code', ci=None, ax=ax[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig10')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H7. European continent conducted the highest number of tests. (TRUE)\n",
    "USA has the highest total vaccination, but Europe vaccinated more proportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh7 = df2[['continent', 'total_tests', 'total_tests_per_thousand', 'year_month']]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "# ax 1\n",
    "sns.barplot(data=auxh7.sort_values('total_tests', ascending=False),\\\n",
    "     x='continent', y='total_tests', ax=ax[0])\n",
    "ax[0].set_title('CONTINENT - TOTAL TESTS')\n",
    "\n",
    "# ax 2\n",
    "sns.lineplot(data=auxh7, x='year_month', y='total_tests_per_thousand',\\\n",
    "    hue='continent', ax=ax[1])\n",
    "ax[1].set_title('CONTINENT - TOTAL TESTS PER THOUSAND')\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig11')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H8. Brazil is more likely to new cases of Covid19. (FALSE)\n",
    "Brazil is the third in number of total cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh8 = df2[['iso_code', 'year_month', 'new_cases', 'new_cases_per_million']]\n",
    "\n",
    "auxh8_gp = auxh8.groupby(['iso_code', 'year_month']).sum().reset_index()\n",
    "\n",
    "# sorting values\n",
    "auxh8_gp1 = auxh8_gp.sort_values('new_cases', ascending=False)\n",
    "auxh8_gp2 = auxh8_gp.sort_values('new_cases_per_million', ascending=False)\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(data=auxh8_gp1.head(50), x='iso_code', y='new_cases')\n",
    "plt.title('NEW CASES')\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(data=auxh8_gp2.head(50), x='iso_code', y='new_cases_per_million')\n",
    "plt.title('NEW CASES PER MILLION')\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(data=auxh8, x='new_cases_per_million', y='new_cases', ci=None)\n",
    "\n",
    "# subplot 4\n",
    "plt.subplot(2,2,4)\n",
    "sns.heatmap(auxh8[['new_cases', 'new_cases_per_million']].corr(method='pearson'), annot=True)\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig12')\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H9. USA is more likely to vaccination. (PERHAPS)\n",
    "USA has the most total vaccination, but not in proportion, large part of the population is still waiting for the vaccine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh9 = df2[['iso_code', 'year_month', 'total_vaccinations', \n",
    "            'total_vaccinations_per_hundred']]\n",
    "\n",
    "auxh9_gp = auxh9[~auxh9.iso_code.str.contains('OWID')].groupby(['iso_code', 'year_month']).sum().reset_index()\n",
    "\n",
    "# sorting values\n",
    "auxh9_gp1 = auxh9_gp.sort_values('total_vaccinations', ascending=False)\n",
    "auxh9_gp2 = auxh9_gp.sort_values('total_vaccinations_per_hundred', ascending=False)\n",
    "auxh9_gp3 = auxh9_gp.sort_values('year_month', ascending=False)\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(data=auxh9_gp1.head(50), x='iso_code', y='total_vaccinations')\n",
    "plt.title('COUNTRIES - TOTAL VACCINATIONS')\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(2,2,2)\n",
    "sns.barplot(data=auxh9_gp2.head(50), x='iso_code', y='total_vaccinations_per_hundred')\n",
    "plt.title('COUNTRIES - TOTAL VACCINATIONS PER HUNDRED')\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(2,2,3)\n",
    "sns.lineplot(data=auxh9, x='year_month', y='total_vaccinations', ci=None)\n",
    "plt.title('TOTAL VACCINATIONS')\n",
    "\n",
    "# subplot 4\n",
    "plt.subplot(2,2,4)\n",
    "sns.heatmap(auxh9[['total_vaccinations', 'total_vaccinations_per_hundred']].corr(method='pearson'), annot=True)\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig13')\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H10. Brasil is more likely Covid19 in South America. (TRUE)\n",
    "In south america Brazil has the highest number of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxh10 = df2[['iso_code', 'continent', 'year_month', 'total_cases', \n",
    "            'total_cases_per_million']]\n",
    "\n",
    "south_america = auxh10.query('continent == \"South America\"')\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(3,1,1)\n",
    "sns.lineplot(data=south_america, x='year_month', y='total_cases', hue='iso_code',\\\n",
    "ci=None)\n",
    "plt.title('SOUTH AMERICA - TOTAL CASES')\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(3,1,2)\n",
    "sns.lineplot(data=south_america, x='year_month', y='total_cases_per_million',\\\n",
    "hue='iso_code', ci=None)\n",
    "plt.title('SOUTH AMERICA - TOTAL VACCINATIONS PER MILLION')\n",
    "\n",
    "# subplot 3\n",
    "plt.subplot(3,1,3)\n",
    "sns.heatmap(auxh10[['total_cases', 'total_cases_per_million']].corr(method='pearson'), annot=True)\n",
    "\n",
    "# adjust\n",
    "plt.tight_layout()\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig14')\n",
    "\n",
    "# show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = gpd.datasets.get_path('naturalearth_lowres')\n",
    "world = gpd.read_file(path)\n",
    "\n",
    "world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = df2.select_dtypes(include='float64')\n",
    "\n",
    "sns.heatmap(num_df.corr(), annot=True, cmap='YlGnBu')\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig15')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high correlation with target variable\n",
    "columns = ['new_cases','new_cases_smoothed', 'total_deaths', 'new_deaths',\n",
    "       'new_deaths_smoothed', 'total_cases_per_million',\n",
    "       'new_cases_per_million', 'new_cases_smoothed_per_million',\n",
    "       'total_deaths_per_million', 'new_deaths_per_million',\n",
    "       'new_deaths_smoothed_per_million']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df2.select_dtypes(exclude='float64')\n",
    "\n",
    "dict_for_corr = {}\n",
    "\n",
    "columns = cat_df.columns.tolist()\n",
    "\n",
    "for column in columns:\n",
    "    dict_for_corr[column] = {}\n",
    "\n",
    "    for column2 in columns:\n",
    "        dict_for_corr[column][column2] = cramers_corrected_stat(cat_df[column], cat_df[column2])\n",
    "\n",
    "corr_cat = pd.DataFrame(dict_for_corr)\n",
    "\n",
    "sns.heatmap(corr_cat, annot=True)\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig16')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Modeling stage of the Team Data Science Process lifecycle\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Determine the optimal data features for the machine-learning model.\n",
    "* Create an informative machine-learning model that predicts the target most accurately.\n",
    "* Create a machine-learning model that's suitable for production.\n",
    "\n",
    "\n",
    "## Model training\n",
    "\n",
    "Depending on the type of question that you're trying to answer, there are many modeling algorithms available. For guidance on choosing the algorithms, see How to choose algorithms for Microsoft Azure Machine Learning. Although this article uses Azure Machine Learning, the guidance it provides is useful for any machine-learning projects.\n",
    "\n",
    "The process for model training includes the following steps:\n",
    "\n",
    "* Split the input data randomly for modeling into a training data set and a test data set.\n",
    "* Build the models by using the training data set.\n",
    "* Evaluate the training and the test data set. Use a series of competing machine-learning algorithms along with the various associated tuning parameters (known as a parameter sweep) that are geared toward answering the question of interest with the current data.\n",
    "* Determine the “best” solution to answer the question by comparing the success metrics between alternative methods.\n",
    "\n",
    "## Artifacts\n",
    "\n",
    "The artifacts produced in this stage include:\n",
    "\n",
    "* Feature sets: The features developed for the modeling are described in the Feature sets section of the Data definition report. It contains pointers to the code to generate the features and a description of how the feature was generated.\n",
    "* Model report: For each model that's tried, a standard, template-based report that provides details on each experiment is produced.\n",
    "* Checkpoint decision: Evaluate whether the model performs sufficiently for production. Some key questions to ask are:\n",
    "    * Does the model answer the question with sufficient confidence given the test data?\n",
    "    * Should you try any alternative approaches? Should you collect additional data, do more feature engineering, or experiment with other algorithms?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset for preparation\n",
    "df3 = pd.read_csv('dataset/df1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to predict Covid19 total cases, so we will drop the columns related to the number of contaminated and killed by Covid19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with null value 60% or more \n",
    "high_nan = ['icu_patients',\n",
    " 'icu_patients_per_million',\n",
    " 'hosp_patients',\n",
    " 'hosp_patients_per_million',\n",
    " 'weekly_icu_admissions',\n",
    " 'weekly_icu_admissions_per_million',\n",
    " 'weekly_hosp_admissions',\n",
    " 'weekly_hosp_admissions_per_million',\n",
    " 'total_vaccinations',\n",
    " 'people_vaccinated',\n",
    " 'people_fully_vaccinated',\n",
    " 'new_vaccinations',\n",
    " 'new_vaccinations_smoothed',\n",
    " 'total_vaccinations_per_hundred',\n",
    " 'people_vaccinated_per_hundred',\n",
    " 'people_fully_vaccinated_per_hundred',\n",
    " 'new_vaccinations_smoothed_per_million']\n",
    "\n",
    "# columns to drop\n",
    "droping = ['total_cases', 'new_cases', 'new_cases_smoothed', \n",
    "        'total_deaths', 'new_deaths', 'new_deaths_smoothed', \n",
    "        'total_cases_per_million', 'new_cases_per_million', \n",
    "        'new_cases_smoothed_per_million', 'total_deaths_per_million', \n",
    "        'new_deaths_per_million', 'new_deaths_smoothed_per_million']\n",
    "\n",
    "# from section 1.6.1.\n",
    "high_outliers = [\n",
    " 'weekly_icu_admissions_per_million',\n",
    " 'new_deaths_smoothed',\n",
    " 'people_fully_vaccinated',\n",
    " 'new_deaths_per_million',\n",
    " 'new_tests_per_thousand',\n",
    " 'hosp_patients',\n",
    " 'population_density',\n",
    " 'population',\n",
    " 'new_cases',\n",
    " 'icu_patients',\n",
    " 'tests_per_case',\n",
    " 'gdp_per_capita',\n",
    " 'people_fully_vaccinated_per_hundred',\n",
    " 'total_tests_per_thousand',\n",
    " 'weekly_icu_admissions',\n",
    " 'people_vaccinated',\n",
    " 'new_tests',\n",
    " 'weekly_hosp_admissions',\n",
    " 'new_tests_smoothed_per_thousand',\n",
    " 'new_cases_smoothed_per_million',\n",
    " 'new_vaccinations_smoothed',\n",
    " 'total_vaccinations_per_hundred',\n",
    " 'new_vaccinations',\n",
    " 'people_vaccinated_per_hundred',\n",
    " 'new_deaths',\n",
    " 'hospital_beds_per_thousand',\n",
    " 'total_cases_per_million',\n",
    " 'total_tests',\n",
    " 'reproduction_rate',\n",
    " 'positive_rate',\n",
    " 'weekly_hosp_admissions_per_million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_outliers2 = list(set(high_outliers) - set(droping) -set(high_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.  NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping null values \n",
    "df3.dropna(subset=['continent'], inplace=True)\n",
    "\n",
    "# replace missing values\n",
    "df3.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of labelencoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# label encoder\n",
    "df3['iso_code'] = le.fit_transform(df3['iso_code'])\n",
    "df3['continent'] = le.fit_transform(df3['continent'])\n",
    "df3['year_month'] = le.fit_transform(df3['year_month'])\n",
    "\n",
    "# transforming datetime into Gregorian calendar\n",
    "df3['date'] = pd.to_datetime(df3['date'])\n",
    "df3['date']=df3['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['total_deaths', 'total_cases', 'iso_code', \n",
    "        'location', 'date', 'tests_units', 'continent']\n",
    "\n",
    "# robust scaler\n",
    "rs = RobustScaler()\n",
    "\n",
    "for i in high_outliers2:\n",
    "    df3[i] = rs.fit_transform(df3[[i]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df3.select_dtypes(include='float64')\n",
    "\n",
    "low_outliers = list(set(numerical.columns) - set(high_outliers2) - set(drop))\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "for i in high_outliers:\n",
    "    df3[i] = mms.fit_transform(df3[[i]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining lists to drop\n",
    "to_drop = list((set(droping) | (set(drop))))\n",
    "to_drop = to_drop.copy()\n",
    "to_drop.extend(high_nan)\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove target variable\n",
    "X_boruta = df3.drop(to_drop, axis=1)\n",
    "\n",
    "# target variable\n",
    "Y_boruta = df3[['total_cases']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_boruta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_boruta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Boruta as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train random forest classifier\n",
    "\n",
    "X_boruta_n = X_boruta.values\n",
    "\n",
    "Y_boruta_n = Y_boruta.values\n",
    "\n",
    "# define random forest regression\n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "# find all relevant features\n",
    "feat_selector.fit(X_boruta_n, Y_boruta_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check selected features\n",
    "cols_selected_boruta = feat_selector.support_.tolist()\n",
    "columns_selected = df3.drop(to_drop, axis=1).loc[:, cols_selected_boruta].columns.tolist()\n",
    "\n",
    "# list with features\n",
    "print(len(columns_selected))\n",
    "print(columns_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An area of acceptance: the features that are here are considered as predictive, so they are kept;\n",
    "* An area of irresolution: Boruta is indecisive about the features that are in this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "acceptance = X_boruta.columns[feat_selector.support_].to_list()\n",
    "irresolution = X_boruta.columns[feat_selector.support_weak_].to_list()\n",
    "print('features in the acceptance:', acceptance)\n",
    "print('features in the irresolution:', irresolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving list\n",
    "boruta_selection = ['total_tests', 'total_tests_per_thousand', 'new_tests_smoothed', 'positive_rate', 'tests_per_case', 'stringency_index', 'population', 'population_density', 'female_smokers', 'male_smokers', 'year_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Random Forest as Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting Model and fit\n",
    "model = RandomForestRegressor().fit(X_boruta, Y_boruta)\n",
    " \n",
    "# plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_boruta.columns)\n",
    "feat_importances.nlargest(30).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing dtype of date\n",
    "\n",
    "x = df3[boruta_selection]\n",
    "y = df3.total_cases\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and fit\n",
    "linear_r = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "lr_pred = linear_r.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "lr_error = error('Linear Regression', y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking results\n",
    "lr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. K-Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit model\n",
    "kn = KNeighborsRegressor().fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "kn_pred = kn.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "kn_error = error('KNeighbors Regressor', y_test, kn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit model \n",
    "rfr = RandomForestRegressor().fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "rfr_pred = rfr.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "rfr_error = error('Random Forest Regression', y_test, rfr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit model\n",
    "xgb = XGBRegressor().fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "xgb_pred = xgb.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "xgb_error = error('XGBoost Regressor', y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0. Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Linear Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function (see section 0.1)\n",
    "linear_r_cv = cross_validation('Linear Regression CV', linear_r, x_train, y_train)\n",
    "linear_r_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. K-Neighbors Regressor CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function (see section 0.1)\n",
    "kn_cv = cross_validation('K-Neighbors CV', kn, x_train, y_train)\n",
    "kn_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Random Forest Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_cv = cross_validation('Random Forest CV', rfr, x_train, y_train)\n",
    "rfr_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. XGBoost Regressor CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = cross_validation('XGBoost CV', xgb, x_train, y_train)\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. Compare Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp = pd.concat([lr_error, kn_error, rfr_error, xgb_error])\n",
    "model_sp.sort_values('RSME').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Real Performance - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rp = pd.concat([linear_r_cv, kn_cv, rfr_cv,  xgb_cv])\n",
    "model_rp.sort_values('RSME').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. Tuning Best Model\n",
    "\n",
    "Since XGBooster is the second best model (in terms of errors) and it took less time to run, I'll follow this cycle using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Tuning Model - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "param_grid = {\n",
    "    'n_estimators' : [100, 200, 500],\n",
    "    'objective': ['reg:squarederror'],\n",
    "    'eta': [0.1],\n",
    "    \"max_depth\": [9],\n",
    "    \"gamma\": [0],\n",
    "    \"reg_lambda\": [10],\n",
    "    \"scale_pos_weight\": [1],\n",
    "    \"subsample\": [1],  # Fix subsample\n",
    "    \"colsample_bytree\": [0.7, 0.8],  # Fix colsample_bytree\n",
    "}\n",
    "xgb_t = XGBRegressor()            \n",
    "xgb_grid = HalvingGridSearchCV(xgb_t,\n",
    "                        param_grid, cv=5, n_jobs=-1,\n",
    "                        verbose=True).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost with best params\n",
    "xgb_tun = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.7, eta=0.1, gamma=0,\n",
    "             gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.100000001, max_delta_step=0, max_depth=9,\n",
    "             min_child_weight=1, monotone_constraints='()',\n",
    "             n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
    "             reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=1,\n",
    "             tree_method='exact', validate_parameters=1, verbosity=None)\\\n",
    "                 .fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "xgb_t_pred = xgb_tun.predict(x_test)\n",
    "\n",
    "# evaluate\n",
    "xgb_tuning_error = error('XGBoost Regressor +', y_test, xgb_t_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuning_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. XGBoost Regressor + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_t_cv = cross_validation('XGBoost + CV', xgb_tun, x_train, y_train)\n",
    "xgb_t_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.0. Final Compare Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1. Single Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp = pd.concat([lr_error, kn_error, rfr_error, xgb_error, xgb_tuning_error])\n",
    "model_sp.sort_values('MAE').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2. Real Performance - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rp = pd.concat([linear_r_cv, kn_cv, rfr_cv, xgb_cv, xgb_t_cv])\n",
    "model_rp.sort_values('MAE').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models\n",
    "\n",
    "pickle.dump(xgb_tun, open('model/xgb_tuned_c2.pkl', 'wb'))\n",
    "pickle.dump(xgb, open('model/xgb_c2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.0. Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading models\n",
    "xgb_tun = pickle.load(open('model/xgb_tuned.pkl', 'rb'))\n",
    "\n",
    "# selecting columns\n",
    "columns_name = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10'] \n",
    "columns_name2 = boruta_selection\n",
    "\n",
    "# zip columns\n",
    "d_columns = dict(zip(columns_name2, columns_name))\n",
    "\n",
    "# changing columns name\n",
    "df4 = df3.copy()\n",
    "df4 = df3.rename(columns = d_columns)\n",
    "\n",
    "# target variable\n",
    "df4['total_cases'] = df3.total_cases\n",
    "\n",
    "# prediction columns\n",
    "df4['total_cases_pred'] = xgb_tun.predict(df4[columns_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. Error and Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get error\n",
    "df4['error'] = df4['total_cases'] - df4['total_cases_pred'] \n",
    "\n",
    "# get error rate\n",
    "df4['error_rate'] = df4['total_cases'] / df4['total_cases_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. Ploting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset for plot\n",
    "df5 = df3.copy()\n",
    "\n",
    "# include columns\n",
    "df5['total_cases_pred'] = df4['total_cases_pred']\n",
    "df5['error'] = df4['error']\n",
    "df5['error_rate'] = df4['error_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "# plot prediction x target\n",
    "sns.lineplot(data=df5, x='year_month', y='total_cases', label='Total Cases',\\\n",
    "     color='red', ax=ax[0,0])\n",
    "sns.lineplot(data=df5, x='year_month', y='total_cases_pred', label='Prediction', ax=ax[0,0])\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title('PREDICTION PERFORMANCE')\n",
    "\n",
    "# plot error\n",
    "sns.lineplot(data=df5, x='year_month', y='error_rate', label='Error Rate', ax=ax[1,0])\n",
    "ax[1,0].axhline(1, linestyle='--')\n",
    "ax[1,0].legend()\n",
    "ax[1,0].set_title('ERROR RATE')\n",
    "\n",
    "# hist error\n",
    "sns.histplot(df5.error, kde=True, ax=ax[0,1])\n",
    "ax[0,1].set_title('ERROR')\n",
    "\n",
    "# plot prediction x error\n",
    "sns.scatterplot(data=df5, x='total_cases_pred', y='error', ax=ax[1,1])\n",
    "ax[1,1].set_title('PREDIC X ERROR')\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig17')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gaussian\n",
    "stats.probplot(df4.error, plot=plt)\n",
    "\n",
    "# save plot\n",
    "plt.savefig('img/fig18')\n",
    "\n",
    "# plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}